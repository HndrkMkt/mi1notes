\section{The backpropagation algorithm}

\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkcyan}{rgb}{0,0.5,0.5}
\definecolor{darkyellow}{rgb}{0.5,0.5,0}
\definecolor{mangenta}{rgb}{1,0,1}

\begin{frame}\frametitle{\secname}

The Backpropagation algorithm is a method for computing gradients by using the chain rule efficiently.
\mode<article>{\eqref{eq:gradient_terms} shows us how the gradient breaks down into (1) an error term and (2) a term dependent on the type of model chosen. Specifically:
}	
\begin{equation*}
	\textcolor{orange}{	\frac{\partial y(\vec{x}; \vec{w})}{
			\partial \mathrm{w}_{ij}^{v'v}}}
    \label{eq:model_term}
\end{equation*}
    
The Backpropagation algorithm handles the computation of the term in \eqref{eq:model_term} for neural networks such as the one depicted in \figref{fig:example_mlp}.

\end{frame}

% -----------------------------------------------------------------------------
\begin{frame} \frametitle{Gradients in neural networks}
    
    \mode<article>{
    The Backpropagation algorithm is not tied to any specific neural architecture. We will therefore attempt to formulate how it works in the general sense.\\
    
    Let $w^{v'v}_{ij}$ be the weight connecting neuron $j$ in layer $v$ to neuron $i$ in layer $v'$, In order to compute $\frac{\partial y(\vec{x}; \vec{w})}{
			\partial \mathrm{w}_{ij}^{v'v}}$ we need to identify the following:
    \begin{itemize}
        \item $P(v, j)$: the set of parent nodes feeding input to neuron $(v, j)$,
        \item $C(v', i)$: the set of children nodes which neuron $(v', i)$ provides with input
    \end{itemize}
    
    $h^{v'}_{i}$ measures the total input arriving at neuron $i$ in layer $v'$. 
    Let $P(v', i)$ describe the set of parent nodes feeding input to neuron $(v', i)$.
    Therefore, $h^{v'}_{i}$ is the weighted sum of this neuron's parent activations:
    }
    
    \begin{figure}[h]
    \centering   
    \includegraphics[height=3cm]{img/section1_fig20_mini_multicolor.pdf}
    \mode<article>{
    \caption{Nodes $(v,j)$ and $(v', i)$ are connected via the weight $w^{v'v}_{ij}$. The bias node for neuron $(v,j)$ is included in the set of its parent nodes.}
    }
    \label{fig:connection_ij}
    \end{figure}
    
    \mode<presentation>{\vspace{-5mm}
    }
    \begin{equation}
            {\color{darkgreen}h_i^{v'}} 
            := 
            \kern-2ex
            \sum_{(\mu,k) \in P(v',\,i)}
            \kern-2ex
            w_{ik}^{v'\mu}\; 
            f_k^\mu\big( {\color{teal} h_k^\mu} \big)   
            \label{eq:total_input_vi}
    \end{equation}
    
    \mode<article>{
    $\mu$ and $k$ in \eqref{eq:total_input_vi} serve as placeholders for the layer and index for each of the parent nodes $P(v', i)$ feeding input to neuron $(v', i)$.
    }
    \mode<presentation>{\vspace{-5mm}
    }
    
    \pause 
    \question{How does $w_{ij}^{v'v}$ contribute to the error?
    \slidesonly{$\frac{\partial y(\vec{x}; \vec{w})}{\partial w_{ij}^{v'v}} = \ldots$
    }}
    
    \mode<article>{
    - The contribution is measured by applying the chain rule:
    }
    \mode<presentation>{\vspace{-9mm}
    }
    
    \begin{eqnarray}
        \frac{\partial y(\vec{x}; \vec{w})}{\partial w_{ij}^{v'v}}
            = \underbrace{\frac{\partial y(\vec{x}; \vec{w})}
                {{\color{darkgreen}\partial h_i^{v'}}} }_{ 
                    \substack{\coloneqq\;{\color{red}\delta_i^{v'}} \\
                    \text{``local error''} \\
                    \text{at neuron} \\
                    (v', i)}
                }
              \cdot 
              \underbrace{\frac{{\color{darkgreen}\partial h_i^{v'}}}
                {\partial w_{ij}^{v'v}}}_{
                    \substack{=\;f_j^v({\color{teal} h_j^v}) \\
                    \text{activity} \\
                    \text{of neuron} \\
                    (v, j)}}
                \label{eq:input_output_terms}
    \end{eqnarray}
    
    \mode<article>{
    Applying the chain rule as done in \eqref{eq:input_output_terms} breaks the contribution of  $w_{ij}^{v'v}$ to the error into two parts:
    %\begin{enumerate}
     %\item Its contribution due to its immediate ``output''. What activity results downstream because of it? In other words, how does $w_{ij}^{v'v}$ influence its \emph{children} in deeper layers?
     %\item Its contribution due to its immediate ``input''. 
    %\end{enumerate}
    }
\end{frame}

% -----------------------------------------------------------------------------
\begin{frame} \frametitle{Gradients in neural networks}
	\only<1,2>{\placeimage{9.5}{1}{img/section1_fig20_mini_multicolor.pdf}{width=4.8cm}}
	\only<3->{\placeimage{9.5}{1}{img/section1_fig20_mini_back_multicolor.pdf}{width=4.8cm}}
    
    \mode<presentation>{ \vspace{11mm} }
	\begin{enumerate}
		\item \textbf{forward propagation}: calculate activities 
				$f_i^{v'}({\color{darkgreen}h_i^{v'}})$
				{\small(parents $\rightarrow$ children)}
				$$	
					{
					%\color{darkgreen}
					h_j^0
					} 
						\;:=\; \mathrm{x}_j \,, 
					\quad 
					{\color{darkgreen}h_i^{v'}}
		   			\,= \kern-2ex\smallsum{(\mu,k) \in P(v',\,i)}{} \kern-2ex
	   			w_{ik}^{v'\mu}\;  f_k^\mu\big( {\color{teal} h_k^\mu} \big) \,,
					\quad
					y(\vec{x}; \vec{w})\,=\, f_1^L({\color{blue}h_1^L})
				$$\\[-2mm]
	\only<1>{	
		\mode<presentation>{
		\begin{center}
		\vspace{2mm}
		\includegraphics[height=4cm]{img/section1_fig14}
		\end{center}
		}
	}
	\only<2>{	
		\mode<presentation>{
		\begin{center}
		\vspace{2mm}
		\includegraphics[height=4cm]{img/section1_fig14_fwd}
		\end{center}
		}
	}
	\only<3->{
		\item \textbf{backpropagation}: calculate ``local errors'' 
				$\color{red}\delta_i^{v'}$
				{\small(children $\rightarrow$ parents)}
				\vspace{-1mm}
				%~ \begin{eqnarray*} 
				%~ \end{eqnarray*}
				\begin{eqnarray*} 
				{\color{red} \delta_1^L}
				&=& \frac{\partial y(\vec{x}; \vec{w})} {{\color{blue}\partial h_1^{L}}}
					= [f_1^{L}]'({\color{blue} h_1^{L}})
							%\;\;\text{for}\;v'=L
							\\
					{\color{red}\delta_i^{v'}} 
					\only<3>{
						&=&  \frac{\partial y(\vec x; \vec w)}{\partial h^{v'}_{i}}
						\qquad \text{local error at neuron }(v', i)\\
						}
					\only<4->{
						&=&  \kern-3ex\sum\limits_{(v''\hspace{-1mm},\,k) \in C(v',\,i)}
						\underbrace{
							\smallfrac{\partial y(\vec{x}; \vec{w})}
								{{\color{blue}\partial  h_k^{v''}}}
							}_{{\color{red}\delta_k^{v''}}} 
						\;\kern1.5ex\cdot \kern-2.5ex\;
						\underbrace{\smallfrac
								{{\color{blue}\partial h_k^{v''}} }
								{{\color{darkgreen}\partial h_i^{v'}}}
							}_{\mathrm{w}_{ki}^{v''v'} \kern-.5ex\cdot\kern.5ex
								[f_i^{v'}]'({\color{darkgreen}h_i^{v'}}) } %\,,
					\kern-3ex=\;\;\; %\underbrace{
							[f_i^{v'}]'({\color{darkgreen}h_i^{v'}}) 
							\kern-3ex\sum_{(v''\hspace{-1mm},\,k) \in C(v',\,i)}\kern-3ex
							{\color{red}\delta_k^{v''}} 
							\mathrm{w}_{ki}^{v'' v'} 
						}
						%}_{{\color{red} \delta_i^L}\;=\;
						%	[f_i^{L}]'({\color{darkgreen} h_i^{L}})
						%	\;\;\text{for}\;v'=L}
				\end{eqnarray*}
		}
		\only<5->{
		\vspace{-1mm}
		\item \textbf {weight update}: using activities and local errors
			$$
				\mathrm{w}_{ij}^{v'v}
				\quad \leftarrow \quad 
				\mathrm{w}_{ij}^{v'v} - \eta \cdot
				\smallfrac{\partial e}{\partial\mathrm{w}_{ij}^{v'v}}
				\quad=\quad \mathrm{w}_{ij}^{v'v} - \eta \cdot
				\smallfrac{\partial e}{\partial y(\vec{x}; \vec{w})} \cdot
				\only<5>{
				\smallfrac{\partial y(\vec{x}; \vec{w})}{\partial\mathrm{w}_{ij}^{v'v}}\hspace{8.5mm}
				}
				\only<6>{
				{\color{red} \delta_i^{v'}} \kern-.5ex\cdot
			   			f_j^v( {\color{darkgreen}h_j^v} )
			   	}
			$$
			}
	\end{enumerate}
	%{\scriptsize
	%Computational complexity: $O(n)$, $n$: number of weights \& thresholds}
\end{frame}

% -----------------------------------------------------------------------------
\definecolor{forward}{rgb}{0,0.7,0}
\definecolor{backward}{rgb}{0.8,0,0}
\begin{frame} \frametitle{Summary of the backpropagation for gradient descent}
	\only<1>{
		\placeimage{10.75}{5.5}{img/MLP_forward.pdf}{width=3.75cm}
		\placeimage{10.75}{8.7}{img/MLP_backward.pdf}{width=3.75cm}
	} \only<2> {
		%\begin{minipage}{4}(11.5,5)
			%{\color{blue}
				%\footnotesize
				%\begin{center}
					%computational and 
					%memory complexity \\[2mm]
					%{\color{red}
						%$\mathcal{O}(n)$, %\quad
					%} \\[2mm]
					%$n$: number of weights
					%i.e.~{\em linear} in the 
					%number of weights
				%\end{center}
			%}
		%\end{minipage}
	}
	
	%\begin{algorithm}[H] 
		%\scriptsize
		\footnotesize
		\DontPrintSemicolon
		initialization of weights and thresholds \\
		\While{stopping criterion not met}{
			$\text{gradient}_{ij}^{v'v} := 0 
					\,, \quad \forall w_{ij}^{v'v}$ \\
			\For{$\alpha \in \{1,\ldots,p\}$}{
				${\color{forward}h_i^0} 
					:= x_i^{(\alpha)} \,, \quad \forall i$ 
				\qquad\qquad // {\color{forward}forward propagation}\\
				\For{$v' \in \{1,\ldots,L\}$}{
					%~ ${\color{forward} h_i^{v'}} 
						%~ := \sum\limits_{\scriptscriptstyle
							%~ (v',i) \in C(v,j)} w_{ij}^{v'v} 
							${\color{forward}h_i^{v'}}
		   			\;\;=\;\; \kern-2ex\smallsum{(\mu,k) \in P(v',i)}{} \kern-2ex
	   			w_{ik}^{v'\mu}\;  \underbrace{f_k^\mu\big( {\color{forward} h_k^\mu} \big) 
								%~ \underbrace{f_j^v({\color{forward} h_j^{v}})
								}_{\kern-2exx_k^{(\alpha)} \;\text{if}\; 
									v'=1\kern-2ex } \,,
							\quad \forall i$
					\vspace{-1.5mm}
				}
				${\color{backward} \delta_1^L} 
					:= [f_1^L]'({\color{forward}h_1^L}) $%\,, \quad \forall i$ 
				\qquad // {\color{backward}backward propagation}\\
				\For{$v' \in \{L-1,\ldots,1\}$} {
					${\color{backward}\delta_i^{v'}} 
						:= [f_{i}^{v'}]'({\color{forward}h_i^{v'}}) 
						\kern-1ex\sum\limits_{(\mu,k) \in C(v',i)}\kern-1ex
						{\color{backward} \delta_k^{\mu}} 
						\, w_{ki}^{\mu v'} \;,
						\quad \forall i$
					\vspace{-2.5mm}
				}
				$\text{gradient}_{ij}^{v'v} := \text{gradient}_{ij}^{v'v}
						+ \frac{\partial e^{(\alpha)}}{\partial y(\vec x; \vec w)}
						 \, {\color{backward}\delta_i^{v'}} 
						 \, f_j^v({\color{forward}h_j^v})
						 \,, \quad \forall w_{ij}^{v'v}$
				\hspace{11mm} // sum
			}
			$\text{gradient}_{ij}^{v'v} := \frac{1}{p} \text{gradient}_{ij}^{v'v}$\\
			% $w_{ij}^{v'v} := w_{ij}^{v'v} - \frac{\hat \eta}{p}
			 $w_{ij}^{v'v} := w_{ij}^{v'v} - \eta
					\, \text{gradient}_{ij}^{v'v} 
					\,, \quad \forall w_{ij}^{v'v}	$
			\hspace{15mm} // gradient descent step
					\vspace{-1.5mm}
		}
		%\caption{Backpropagation in feedforward networks}
	%\end{algorithm}
\end{frame}
