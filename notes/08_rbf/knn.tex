\section{Non-parametric classification}

\subsection{The setting}

\begin{frame}\frametitle{\subsecname for M-way classification}

\mode<article>{
Specifying the data and model for a multi-class classification with $M$ classes (i.e. $M$-way classification)
}

\begin{itemize}
	\item \underline{Data}:\\

	\begin{equation*}
	\Big\{ \left(\vec x^{(\alpha)}, \vec y^{(\alpha)}_{T} \right) \Big\}\,
	\end{equation*}

	where 
	\begin{itemize}
	\item[] $\alpha = 1,\ldots,p$ and
	\item[]$\vec y_T^{(\alpha)} \in \{0, 1\}^M$ with $\sum_{c=1}^{M} (y_{T})_c = 1$ (one-hot encoding of class labels),
	\end{itemize}

	\pause

	\item \underline{Model}:\\

	\begin{equation*}
	\vec y(\vec x) \in \R^M 
	\end{equation*}
	with 
	\begin{itemize}
	\item[] $\sum_{c=1}^{M} y_c(\vec x) = 1$ and
	\item[] $y_c(\vec x)\,\ge\,0\; \forall c$
	\end{itemize}
	
	Use $\vec y(\vec x)$ to give
	\begin{itemize}
	\item probabilities of the predicted class (e.g. $y_5(\vec x) = 0.75\; \leadsto$ class ``5'' with 75\% probability.
	\item hard predictions/decisions: $\argmax_{c=1,\ldots,M} y_c(\vec x)$
	\end{itemize}

\end{itemize}

\end{frame}

\subsection{k nearest neighbor}


\begin{frame}\frametitle{\subsecname}

Prediction follows the majority vote of the $k$ nearest neighbors around the query point.

\begin{figure}[ht]
     \centering
	\includegraphics[width=0.2\textwidth]{img/section4_fig11_K2}
     \mode<article>{
	\caption{Basic RNN architecture}
	}
	\label{fig:rnn} 
\end{figure}

\end{frame}

\begin{frame}\frametitle{\subsecname}

Let $k$NN$(\vec x)$ be the indices $\{\beta_1, \beta_2,\ldots,\beta_k\}$ of the $k$ data points closest to $\vec x$ w.r.t. Euclidean norm:

\begin{equation}
\beta_j = 
\argmin_{\alpha \in \{1,\ldots,p\} \textbackslash \{\beta_{1},\ldots,\beta_{j-1}\}}
\lVert \vec x^{(\alpha)} - \vec x\rVert_{2}
\end{equation}

The $k$NN classifier (for fixed $k~\corresponds$ hyperparameter) is defined by

\begin{equation}
\vec y(\vec x) \frac{1}{k} \sum_{\beta \in k\mathrm{NN}(\vec x)} \vec y_{T}^{(\beta)}
\label{eq:knn_classifier}
\end{equation}

$\vec y(\vec )$ in \eqref{eq:knn_classifier} is effectively an arithmetic average of the labels in the neighborhood.

\end{frame}

\begin{frame}
Example k=3, M=4

\begin{equation}
\vec y(\vec x) = \frac{1}{3}
\left\lbrack
\right\rbrack
\end{equation}
    
\end{frame}
