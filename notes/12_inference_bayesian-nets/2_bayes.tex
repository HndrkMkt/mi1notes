\section{Independence}

\definecolor{darkgreen}{rgb}{0,0.6,0}

\mode<presentation>{
\begin{frame} 
    \begin{center} \huge
        \secname
    \end{center}
    \begin{center}   
    come up with a more compact representation of the full joint distribution
    \end{center}
\end{frame}
}

\subsection{Motivation}

\begin{frame}\frametitle{\secname:~\subsecname}

The full joint distribution has everything we need to perform inference,
but it scales badly with more and more variables (e.g. adding weather (e.g. \# of weather conditions = 4) leads to a table with $2\times2\times2\times4=32$ entries)

\pause

\question{Should the weather be influenced by a cavity?}

\pause

\slidesonly{\vspace{-5mm}}

\begin{equation}
P(cloudy | toothache, catch, cavity) \stackrel{!}{=} P(cloudy)
\end{equation}

\pause

\question{Should the weather have any influence on cavities, toothache or the dentist?}

\pause

\notesonly{
-No, this independence can be formulated by:
}

\slidesonly{\vspace{-5mm}}

\begin{align}
P(toothache, catch, cavity, Weather) &\\
\stackrel{!}{=} P(toothache, &catch, cavity)P(Weather)
\end{align}


The 32 elements in the table can be split into \notesonly{the original} 8 + 4 \notesonly{(new table with 4 entries for the weather)}.

\end{frame} 

\begin{frame}\secname:~\subsecname

Extreme case: $n$ independent coin flips: $2^n$ combinations. Independence allows us to reduce this to $n \times$ single-variable distributions.

\end{frame}

\subsection{Bayes' theorem}

\mode<presentation>{
\begin{frame} 
    \begin{center} \huge
        \subsecname
    \end{center}
    
    \begin{equation*}
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
\end{equation*}
    
    \begin{center}   
   How do we get there? - product rule\\
     Why is it important? $\Rightarrow$ Conditional independence
    \end{center}
\end{frame}
}

\begin{frame}\frametitle{\subsecname}

Start with the product rule:

\begin{align}
P(X|Y)P(Y) = P(Y,X) &= P(X,Y) \visible<2->{= P(Y|X)P(X)\\
P(X|Y)P(Y) &= P(Y|X)P(X)}
\visible<3->{
\intertext{solve for $P(Y|X)$}
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
}
%\tag{Bayes' theorem}
\label{eq:bayes}
\end{align}

\end{frame}

\begin{frame}\frametitle{How to read Bayes' theorem}
\notesonly{
How to read Bayes' theorem:
}

\mode<presentation>{
    \begin{equation*}
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
\end{equation*}
}

\pause

\begin{equation}
\underbrace{P(cause|effect)}_{\text{diagnositc}} = \frac{\overbrace{P(effect|cause)}^{\text{causal}}P(cause)}{P(effect)}
\end{equation}

If $P(effect)$ is missing, we can still normalize using:

\begin{equation}
P(Y|X) = \alpha P(X|Y)P(Y)
\end{equation}

$\alpha$ is chosen to whatever will make the entries in $P(Y|X)$ sum up to $1$.


\end{frame}

\subsection{Using Bayes' theorem}

\begin{frame}\frametitle{\subsecname}

\only<1>{
\begin{equation}
P(Y|X) = \alpha P(X|Y)P(Y)
\end{equation}

\begin{align}
P(Cavity|toothache=true, catch=true) &\\ 
=\alpha \underbrace{P(toothache=true, catch=true}_{\circledast} &| Cavity)P(Cavity)
\label{eq:applybayes}
\end{align}

$\circledast$ similar scaling problem, not much better then the case of using the full joint distribution.

}

\only<1,2>{

\question{Can we use independence to further mitigate the scaling problem here?}

}

\only<2>{

\begin{center}
$Toothache$ and $Catch$ are not independent.\\
But\\
$Toothache$ given $cavity$ is independent of $Catch$ given $cavity$
\end{center}

\begin{itemize}
\item $toothache$ depends on $cavity$: nerves \& tolerance for pain.
\item $catch$ depends on $cavity$: dependent on how skillful the dentist is.
\item ``nerves \& tolerance for pain'' is \underline{independent} of ``how skillful the dentist is''.
\end{itemize}

%TODO figure

}




\end{frame}

\begin{frame}

\mode<presentation>{
\slidesonly{\vspace{-5mm}
}

\begin{align}
P(Cavity|toothache=true, catch=true) &\\ 
=\alpha \underbrace{P(toothache=true, catch=true}_{\circledast} &| Cavity)P(Cavity)
\label{eq:applybayes}
\end{align}

\begin{itemize}
\item $toothache$ depends on $cavity$: nerves \& tolerance for pain.
\item $catch$ depends on $cavity$: dependent on how skillful the dentist is.
\item ``nerves \& tolerance for pain'' is \underline{independent} of ``how skillful the dentist is''.
\end{itemize}
}

Therefore:
\slidesonly{\vspace{-5mm}
}

\begin{align}
P(toothache=\text{true}, catch=\text{true} | cavity) &\\
= P(toothache=\text{true}| cavity) &P(catch=\text{true}| cavity)
\label{eq:condindep}
\end{align}

i.e. \emph{conditional} independence of $toothache$ and $catch$ \underline{given} $cavity$

\pause

Plugging \eqref{eq:condindep} into \eqref{eq:applybayes} yields:

\slidesonly{\vspace{-4mm}}

\begin{align}
P(toothache, catch | Cavity) &\\
= \alpha P(toothache| Cavity) &
P(catch| Cavity) P(Cavity)
\end{align}

i.e. \emph{conditional} independence of $toothache$ and $catch$ \underline{given} $Cavity$
\end{frame}

\subsection{Naive Bayes}

\begin{frame}\frametitle{\subsecname}

An alternate expression for the full joint distribution by exploiting conditional independence.

We start with the product rule:

\begin{equation}
P(X,Y) = P(X|Y)P(Y)    
\end{equation}

\begin{equation}
P(Toothache,Catch,Cavity) = P(Toothache,Catch|Cavity) \underbrace{P(Cavity)}_{\text{prior}} 
\end{equation}

Exploiting conditional independence yields:

\begin{align}
P(Toothache,Catch,Cavity) &\\
= P(Toothache|Cavity) &P(Catch|Cavity) \underbrace{P(Cavity)}_{\text{prior}} 
\end{align}

\end{frame}

\begin{frame}\frametitle{\subsecname}

\mode<presentation>{
\begin{align*}
P(Toothache,Catch,Cavity) &\\
= P(Toothache|Cavity) &P(Catch|Cavity) \underbrace{P(Cavity)}_{\text{prior}} 
\end{align*}
}

In terms of cause and effect:

\begin{align}
P(Cause,Effect_1,Effect_2,\ldots,Effect_n) &\\
= \underbrace{P(Cause)}_{\text{prior}} &\prod_{i=1}^{n} P(Effect_{i}|Cause)
\end{align}

Complexity reduced!

\end{frame}
