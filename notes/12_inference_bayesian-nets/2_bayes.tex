\section{Independence}

\definecolor{darkgreen}{rgb}{0,0.6,0}

\mode<presentation>{
\begin{frame} 
    \begin{center} \huge
        \secname
    \end{center}
    \begin{center}   
    Essentially $\varepsilon$-SVR but treat $\varepsilon$ as a primal variable instead of a hyperparameter.
    \end{center}
\end{frame}
}

\subsection{Motivation}

\begin{frame}\frametitle{\secname:~\subsecname}

The full joint distribution has everything we need to perform inference,
but it scales badly with more and more variables (e.g. adding weather (e.g. \# of weather conditions = 4) leads to a table with $2\times2\times2\times4=32$ entries)

\question{Should the weather be influenced by a cavity?}

\begin{equation}
P(cloudy | toothache, catch, cavity) \stackrel{!}{=} P(cloudy)
\end{equation}

\question{Should the weather have any influence on cavities, toothache or the dentist?}

-No, this independence can be formulated by:

\begin{equation}
P(toothache, catch, cavity, Weather) \stackrel{!}{=} P(toothache, catch, cavity)P(Weather)
\end{equation}

The 32 elements in the table can be split into the original 8 + 4 (new table with 4 entries for the weather)


Extreme case: $n$ independent coint flips: $2^n$ combinations. Independence allows us to reduce this to $n \times$ single-variable distributions. 

\end{frame}

\subsection{Bayes' theorem}

\begin{frame}

start with the product rule:

\begin{align}
P(X|Y)P(Y) = P(Y,X) &= P(X,Y) = P(Y|X)P(X)\\
P(X|Y)P(Y) &= P(Y|X)P(X)
\intertext{solve for $P(Y|X)$}
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
\label{eq:bayes}
\end{align}

How to read this:

\begin{equation}
\underbrace{P(cause|effect)}_{\text{diagnositc}} = \frac{\overbrace{P(effect|cause)}^{\text{causal}}P(cause)}{P(effect)}
\end{equation}

If $P(effect)$ is missing, we can still normalize using:

\begin{equation}
P(Y|X) = \alpha P(X|Y)P(Y)
\end{equation}

$\alpha$ is chosen to whatever will make the entries in $P(Y|X)$ sum up to $1$.

\end{frame}

\subsection{Using Bayes' theorem}

\begin{frame}\frametitle{\subsecname}

\begin{equation}
P(Y|X) = \alpha P(X|Y)P(Y)
\end{equation}

\begin{equation}
P(Cavity|toothache=true, catch=true) = \alpha \underbrace{P(toothache=true, catch=true}_{*} | Cavity)P(Cavity)
\label{eq:applybayes}
\end{equation}

$*$ similar scaling problem, not much better then the case of using the full joint distribution.

\question{Can we use independence tp further mitigate the scaling problem here?}

\begin{center}
Toothache and Catch are not independent.\\
But\\
Toothache given cavity is independent of catch given cavity
\end{center}

toothache depends on cavity: nerves, tolerance for pain

catch depends on a cavity: dependent on how skillfull the dentist is

nerves, tolerance for pain is independent of how skillfull the dentist is

TODO figure




\end{frame}

\begin{frame}


toothache depends on cavity: nerves, tolerance for pain

catch depends on a cavity: dependent on how skillfull the dentist is

nerves, tolerance for pain is independent of how skillfull the dentist is


Therefore:

\begin{equation}
P(toothache=true, catch=true | Cavity) = P(toothache=true| Cavity) P(catch=true| Cavity)
\label{eq:condindep}
\end{equation}

i.e. \emph{conditional} independence of toothache and catch \underline{given} cavity

Plugging \eqref{eq:condindep} into \eqref{eq:applybayes} yields:

\begin{equation}
P(toothache=true, catch=true | Cavity) = \alpha P(toothache=true| Cavity) P(catch=true| Cavity) P(Cavity)
\end{equation}

i.e. \emph{conditional} independence of toothache and catch \underline{given} cavity
\end{frame}

\subsection{Naive Bayes}

\begin{frame}\frametitle{\subsecname}

We need to an alternate expression for the full joint distribution.

We start with the product rule:

\begin{equation}
P(X,Y) = P(X|Y)P(Y)    
\end{equation}

\begin{equation}
P(Toothache,Catch,Cavity) = P(Toothache,Catch|Cavity)\underbrace{P(Cavity)}_{\text{prior}} 
\end{equation}

Exploit conditional independence:

\begin{equation}
P(Toothache,Catch,Cavity) = P(Toothache|Cavity)P(Catch|Cavity) \underbrace{P(Cavity)}_{\text{prior}} 
\end{equation}

In terms of cause and effect:

\begin{equation}
P(Cause,Effect_1,Effect_2,\ldots,Effect_n) = \underbrace{P(Cause)}_{\text{prior}} \prod_{i=1}^{n} P(Effect_{i}|Cause)
\end{equation}

\question{Which is the effect? Which is the cause}

Complexity reduced!

\end{frame}
